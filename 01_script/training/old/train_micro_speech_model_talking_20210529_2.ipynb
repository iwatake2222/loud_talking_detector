{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_micro_speech_model_talking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwatake2222/pico-loud_talking_detector/blob/master/01_script/training/train_micro_speech_model_talking_20210529_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y52U_k7Sho0w"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4-CY_TCZZS"
      },
      "source": [
        "# Train a Simple Audio Recognition Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaFfr7DHRmGF"
      },
      "source": [
        "This notebook demonstrates how to train a 20 kB [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) model to recognize keywords in speech.\n",
        "\n",
        "The model created in this notebook is used in the [micro_speech](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) example for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview).\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zai5GNyXji1"
      },
      "source": [
        "## Prepare my dataset (iwatake2222)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuPZ7HzIE7RO"
      },
      "source": [
        "### Download dataset (iwatake2222)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj1b7Mo08RL5"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "\n",
        "CLIP_DURATION = 10 * 1000\n",
        "\n",
        "DATASET_DIR =  \"dataset/\"\n",
        "!rm -rf {DATASET_DIR} && mkdir -p {DATASET_DIR}\n",
        " \n",
        "def download(file_id, file_name):\n",
        "  subprocess.run([\"curl\", \"-sc\", \"/tmp/cookie\", f\"https://drive.google.com/uc?export=download&id={file_id}\"])\n",
        "  cmd = [\"awk\", \"/_warning_/ {print $NF}\", \"/tmp/cookie\"]\n",
        "  code = subprocess.run(cmd, shell=False, stdout=subprocess.PIPE, check=True).stdout.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
        "  subprocess.run([\"curl\", \"-Lb\", \"/tmp/cookie\", f\"https://drive.google.com/uc?export=download&confirm={code}&id={file_id}\", \"-o\", f\"{file_name}\"])\n",
        " \n",
        "# def extract_dataset(file_id, file_name, dataset_dir):\n",
        "#   download(file_id, file_name)\n",
        "#   subprocess.run([\"unzip\", \"-o\", f\"{file_name}\"])\n",
        "#   data_path = os.path.splitext(os.path.basename(file_name))[0]\n",
        "#   subprocess.run(f\"cp -rf {data_path}/* {dataset_dir}/.\", shell=True)\n",
        "def extract_dataset(file_id, file_name, dataset_dir):\n",
        "  if not os.path.exists(file_name):\n",
        "    download(file_id, file_name)\n",
        "  subprocess.run([\"tar\", \"xzvf\", file_name, \"--strip\", \"1\", \"-C\", dataset_dir])\n",
        " \n",
        "''' Download my dataset '''\n",
        "# AudioSet (use not_talking data only)\n",
        "extract_dataset(\"1wLit745TX4rw_KgUJULuhZxXETC58fgd\", \"balanced_train_segments.tgz\", DATASET_DIR)\n",
        "# extract_dataset(\"1l5pj8DO0rreT-OimYdZAf2mJdmV_6nYu\", \"eval_segments.tgz\", DATASET_DIR)\n",
        "!rm -rf {DATASET_DIR}/ambiguous\n",
        "!rm -rf {DATASET_DIR}/talking/*\n",
        " \n",
        "# My data\n",
        "extract_dataset(\"13HC_vZeXwqpz4eKZliLKomvTBuYW1-jt\", \"music_pops.tgz\", DATASET_DIR + \"not_talking\")\n",
        "extract_dataset(\"1tlkCJ2RnhapPmPeIhd09dzTD9xrovyV4\", \"music.tgz\", DATASET_DIR + \"not_talking\")\n",
        "extract_dataset(\"12_z34bHB1bR3QbTM-rzJOq8Fol1G3kpj\", \"yoshimoto.tgz\", DATASET_DIR + \"talking\")\n",
        " \n",
        "!rm -rf temp && mkdir temp\n",
        "extract_dataset(\"1MhhJJxqCdd33gxgwBkuTJzABURjOsUNP\", \"my_jp_talk.tgz\", \"temp\")\n",
        "!find temp -name *.wav -exec mv {} temp \\;\n",
        "!mv temp/*.wav {DATASET_DIR}\"/talking/.\"\n",
        "!rm -rf temp\n",
        " \n",
        "# WANTED_WORDS_LIST = [\"talking\", \"not_talking\", \"ambiguous\"]\n",
        "WANTED_WORDS_LIST = [\"talking\", \"not_talking\"]\n",
        " \n",
        "# ''' Download background_noise '''\n",
        "# !curl -O \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "# !tar xzf speech_commands_v0.02.tar.gz ./_background_noise_\n",
        "# !cp -r _background_noise_ {DATASET_DIR}/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M7I36qb0i4m"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "''' Adjust the number of dataset '''\n",
        "def get_file_num(directory):\n",
        "  return len([name for name in os.listdir(directory) if os.path.isfile(directory + \"/\" + name)])\n",
        "\n",
        "def choose_random_data(DATASET_DIR, label, data_num):\n",
        "  org_path = DATASET_DIR + \"/\" + label + \"/\"\n",
        "  tmp_path = DATASET_DIR + \"/temp_\" + label + \"/\"\n",
        "  os.makedirs(tmp_path, exist_ok=True)\n",
        "  subprocess.run(f\"mv {org_path}/* {tmp_path}/.\", shell=True)\n",
        "\n",
        "  files = [r.split('/')[-1] for r in glob.glob(tmp_path + \"/*.wav\")]\n",
        "  for i in range(data_num):\n",
        "    chosen_file_name = random.choice(files)\n",
        "    files.remove(chosen_file_name)\n",
        "    chosen_file_path = tmp_path + chosen_file_name\n",
        "    subprocess.run(f\"mv {chosen_file_path} {org_path}/.\", shell=True)\n",
        "  subprocess.run(f\"rm -rf {tmp_path}\", shell=True)\n",
        "  !rm -rf {tmp_path}\n",
        "\n",
        "# data_num = get_file_num(DATASET_DIR + \"talking\")\n",
        "# print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")), str(get_file_num(DATASET_DIR + \"ambiguous\")))\n",
        "# choose_random_data(DATASET_DIR, \"not_talking\", data_num)\n",
        "# choose_random_data(DATASET_DIR, \"ambiguous\", data_num)\n",
        "# print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")), str(get_file_num(DATASET_DIR + \"ambiguous\")))\n",
        "print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLW7XXU93Wgp"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "def separate_wav(target_dir, output_dir, sampling_rate=16000, output_duration_time=5,delete_original=False):\n",
        "    output_duration_sample = int(sampling_rate * output_duration_time)\n",
        "\n",
        "    ''' Process for selected input files '''\n",
        "    wav_path_list = glob.glob(target_dir + \"/*.wav\")\n",
        "    for wav_path in wav_path_list:\n",
        "        basename, ext = os.path.splitext(os.path.basename(wav_path))\n",
        "\n",
        "        try:\n",
        "            data, sr = librosa.core.load(wav_path, sr=sampling_rate, mono=True)\n",
        "        except:\n",
        "            continue\n",
        "        duration_sample = len(data)\n",
        "        index = 0\n",
        "        while (index + 0.5) * output_duration_sample <= duration_sample:\n",
        "            if (index + 1) * output_duration_sample <= duration_sample:\n",
        "                data_out = data[index * output_duration_sample : (index + 1) * output_duration_sample]\n",
        "            else:\n",
        "                data_out = data[duration_sample - output_duration_sample : duration_sample]\n",
        "            output_path = output_dir + \"/\" + basename + \"_\" + f\"{index:02}\" + \".wav\"\n",
        "            soundfile.write(output_path, data_out, samplerate=sampling_rate, subtype=\"PCM_16\")\n",
        "            index += 1\n",
        "        if delete_original:\n",
        "            os.remove(wav_path)\n",
        "        rest_data_sample = duration_sample - (index * output_duration_sample)\n",
        "        # if rest_data_sample > 0:\n",
        "        #     print(f\"Warning: data dropped, {basename}, {str(rest_data_sample)}\")\n",
        "\n",
        "separate_wav(DATASET_DIR + \"/talking/.\", DATASET_DIR + \"/talking/.\", sampling_rate=16000, output_duration_time=CLIP_DURATION/1000, delete_original=True)\n",
        "separate_wav(DATASET_DIR + \"/not_talking/.\", DATASET_DIR + \"/not_talking/.\", sampling_rate=16000, output_duration_time=CLIP_DURATION/1000, delete_original=True)\n",
        "\n",
        "print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv7KX6BvFEiQ"
      },
      "source": [
        "### Add noise and background (iwatake2222)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QQG8aa7FC0e"
      },
      "source": [
        "!apt install librosa soundfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAvSmGPeFTU_"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "def clear_last_sep(dir):\n",
        "    dir.replace(os.sep,'/')\n",
        "    if dir[-1] == \"/\":\n",
        "        dir = dir[:-1]\n",
        "    return dir\n",
        "\n",
        "def add_noise(target_dir, noise_dir, output_dir, signature_text=\"\", sampling_rate=16000, process_ratio=0.3, original_volume=1.0, noise_volume=1.0):\n",
        "    if signature_text == \"\":\n",
        "        signature_text = os.path.splitext(os.path.basename(clear_last_sep(noise_dir)))[0]\n",
        "\n",
        "    ''' Make sure the shuffling is deterministic for reproduce '''\n",
        "    random.seed(1234)\n",
        "\n",
        "    ''' Read noise data as array '''\n",
        "    noise_list = []\n",
        "    noise_wav_path_list = glob.glob(noise_dir + \"/*.wav\")\n",
        "    for noise_wav_path in noise_wav_path_list:\n",
        "        data, sr = librosa.core.load(noise_wav_path, sr=sampling_rate, mono=True)\n",
        "        noise_list.append(data)\n",
        "\n",
        "    ''' Process for selected input files '''\n",
        "    wav_path_list = glob.glob(target_dir + \"/*.wav\")\n",
        "    random.shuffle(wav_path_list)\n",
        "    wav_path_list = wav_path_list[:int(len(wav_path_list) * process_ratio)]\n",
        "    for wav_path in wav_path_list:\n",
        "        basename, ext = os.path.splitext(os.path.basename(wav_path))\n",
        "        output_path = output_dir + \"/\" + basename + \"_\" + signature_text + \".wav\"\n",
        "\n",
        "        data, sr = librosa.core.load(wav_path, sr=sampling_rate, mono=True)\n",
        "        duration_sample = len(data)\n",
        "\n",
        "        noise = random.choice(noise_list)\n",
        "        start_sample = int(random.uniform(0, len(noise) - duration_sample - 1))\n",
        "        data = data * original_volume + noise[start_sample:start_sample + duration_sample] * noise_volume\n",
        "\n",
        "        soundfile.write(output_path, data, samplerate=sampling_rate, subtype=\"PCM_16\")\n",
        "\n",
        "def create_noise(noise_dir, output_dir, signature_text=\"\", sampling_rate=16000, duration_time=10, output_number_of_file=10, noise_volume=1.0):\n",
        "    if signature_text == \"\":\n",
        "        signature_text = os.path.splitext(os.path.basename(clear_last_sep(noise_dir)))[0]\n",
        "\n",
        "    ''' Make sure the shuffling is deterministic for reproduce '''\n",
        "    random.seed(1234)\n",
        "\n",
        "    ''' Read noise data as array '''\n",
        "    noise_list = []\n",
        "    noise_wav_path_list = glob.glob(noise_dir + \"/*.wav\")\n",
        "    for noise_wav_path in noise_wav_path_list:\n",
        "        data, sr = librosa.core.load(noise_wav_path, sr=sampling_rate, mono=True)\n",
        "        noise_list.append(data)\n",
        "\n",
        "    duration_sample = int(duration_time * sampling_rate)\n",
        "    ''' Process to create files'''\n",
        "    for i in range(output_number_of_file):\n",
        "        noise = random.choice(noise_list)\n",
        "        start_sample = int(random.uniform(0, len(noise) - duration_sample - 1))\n",
        "        data = noise[start_sample:start_sample + duration_sample] * noise_volume\n",
        "        output_path = output_dir + \"/\" + signature_text + f\"_{i:05}.wav\"\n",
        "        soundfile.write(output_path, data, samplerate=sampling_rate, subtype=\"PCM_16\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks77gb92Fcvo"
      },
      "source": [
        "''' Download noise data '''\n",
        "BACKGROUND_DIR = \"background/\"\n",
        "NOISE_DIR = \"noise/\"\n",
        "!rm -rf {BACKGROUND_DIR} && mkdir -p {BACKGROUND_DIR}\n",
        "!rm -rf {NOISE_DIR} && mkdir -p {NOISE_DIR}\n",
        "extract_dataset(\"19vFtkVp1d_e8lBBnfnD8K8JohBPcPM_H\", \"background.tgz\", BACKGROUND_DIR)\n",
        "extract_dataset(\"12R0jtfXr6cGWYYZaTcV2B0nJvXV8AV-F\", \"mic_noise.tgz\", NOISE_DIR)\n",
        "\n",
        "''' Add noise files into \"not_talking\" '''\n",
        "# Create noise data after separatint test data\n",
        "# noise_num = int(get_file_num(DATASET_DIR + \"not_talking\") * 0.2)\n",
        "# create_noise(BACKGROUND_DIR, DATASET_DIR + \"not_talking\", duration_time=CLIP_DURATION/1000, output_number_of_file=noise_num, noise_volume=1.0)\n",
        "# create_noise(NOISE_DIR, DATASET_DIR + \"not_talking\", duration_time=CLIP_DURATION/1000, output_number_of_file=noise_num, noise_volume=1.0)\n",
        "\n",
        "''' Mix noise and \"talking\" '''\n",
        "# the order is important (mix background, then mix noise)\n",
        "add_noise(DATASET_DIR + \"talking\", BACKGROUND_DIR, DATASET_DIR + \"talking\", process_ratio=0.5, original_volume=1.0, noise_volume=1.0)\n",
        "add_noise(DATASET_DIR + \"talking\", NOISE_DIR, DATASET_DIR + \"talking\", process_ratio=0.5, original_volume=1.0, noise_volume=1.0)\n",
        "print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWd_qA8kMwe9"
      },
      "source": [
        "### Separate test data\n",
        "Manually separate test data from training data. because the training script randomly pick-up training/validation/test data and it causes data leakage.\n",
        "Copy wav files which has similar prefix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puvGiSW9JAg9"
      },
      "source": [
        "import shutil\n",
        "\n",
        "DATASET_TEST_DIR =  \"dataset_test/\"\n",
        "!rm -rf {DATASET_TEST_DIR} && mkdir -p {DATASET_TEST_DIR} && mkdir -p {DATASET_TEST_DIR}/talking && mkdir -p {DATASET_TEST_DIR}/not_talking\n",
        "def move_test_data(src_path, dst_path, ratio):\n",
        "  random.seed(984983)\n",
        "  num_to_be_mv = int(len(glob.glob(src_path + \"/*.wav\")) * ratio)\n",
        "  while num_to_be_mv > 0:\n",
        "    org_file_list = glob.glob(src_path + \"/*.wav\")\n",
        "    basename, ext = os.path.splitext(os.path.basename(random.choice(org_file_list)))\n",
        "    # Check the first letters to identigy if the files are the same (see longer is filename is date)\n",
        "    num_to_identify_scene = 5\n",
        "    if basename[:8].isdigit():\n",
        "      num_to_identify_scene = 13\n",
        "    target_file_list = glob.glob(src_path + \"/\" + basename[:num_to_identify_scene] + \"*.wav\")\n",
        "    for target_file in target_file_list:\n",
        "      shutil.move(target_file, dst_path + \"/.\")\n",
        "      num_to_be_mv -= 1\n",
        "\n",
        "move_test_data(DATASET_DIR + \"talking\", DATASET_TEST_DIR + \"talking\", 0.1)\n",
        "move_test_data(DATASET_DIR + \"not_talking\", DATASET_TEST_DIR + \"not_talking\", 0.1)\n",
        "# move_test_data(DATASET_TEST_DIR + \"talking\", DATASET_DIR + \"talking\", 1.0)  # revert\n",
        "# move_test_data(DATASET_TEST_DIR + \"not_talking\", DATASET_DIR + \"not_talking\", 1.0) # revert\n",
        "print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")))\n",
        "print(str(get_file_num(DATASET_TEST_DIR + \"talking\")), str(get_file_num(DATASET_TEST_DIR + \"not_talking\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AjjDkt1g7oG"
      },
      "source": [
        "''' Add noise data to training data (this sould be after separating test data) '''\n",
        "noise_num = int(get_file_num(DATASET_DIR + \"not_talking\") * 0.2)\n",
        "create_noise(BACKGROUND_DIR, DATASET_DIR + \"not_talking\", duration_time=CLIP_DURATION/1000, output_number_of_file=noise_num, noise_volume=1.0)\n",
        "create_noise(NOISE_DIR, DATASET_DIR + \"not_talking\", duration_time=CLIP_DURATION/1000, output_number_of_file=noise_num, noise_volume=1.0)\n",
        "print(str(get_file_num(DATASET_DIR + \"talking\")), str(get_file_num(DATASET_DIR + \"not_talking\")))\n",
        "print(str(get_file_num(DATASET_TEST_DIR + \"talking\")), str(get_file_num(DATASET_TEST_DIR + \"not_talking\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVtYN4nlCft"
      },
      "source": [
        "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and set **Hardware accelerator: GPU**. Training 15,000 iterations will take 1.5 - 2 hours on a GPU runtime.\n",
        "\n",
        "## Configure Defaults\n",
        "\n",
        "**MODIFY** the following constants for your specific use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ludfxbNIaegy"
      },
      "source": [
        "# A comma-delimited list of the words you want to train for.\n",
        "# The options are: yes,no,up,down,left,right,on,off,stop,go\n",
        "# All the other words will be used to train an \"unknown\" label and silent\n",
        "# audio data with no spoken words will be used to train a \"silence\" label.\n",
        "\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "# WANTED_WORDS = \"yes,no\"\n",
        "WANTED_WORDS = \",\".join(WANTED_WORDS_LIST)\n",
        "# The number of steps and learning rates can be specified as comma-separated\n",
        "# lists to define the rate at each stage. For example,\n",
        "# TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001\n",
        "# will run 12,000 training loops in total, with a rate of 0.001 for the first\n",
        "# 8,000, and 0.0001 for the final 3,000.\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "TRAINING_STEPS = \"12000,3000\"\n",
        "# TRAINING_STEPS = \"5000,1000\"\n",
        "LEARNING_RATE = \"0.001,0.0001\"\n",
        "\n",
        "# Calculate the total number of steps, which is used to identify the checkpoint\n",
        "# file name.\n",
        "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Training these words: %s\" % WANTED_WORDS)\n",
        "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
        "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
        "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgeOpvY9pAi"
      },
      "source": [
        "**DO NOT MODIFY** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd1iM1o2ymvA"
      },
      "source": [
        "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
        "# to ensure that we have equal number of samples for each label.\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1\n",
        "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "equal_percentage_of_training_samples = 0\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "# Constants which are shared during training and inference\n",
        "PREPROCESS = 'micro'\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "# CLIP_DURATION = 10000\n",
        "WINDOW_SIZE = 30\n",
        "WINDOW_STRIDE = 20\n",
        "FEATURE_BIN_COUNT = 40\n",
        "# MODEL_ARCHITECTURE = 'conv' # Other options include: single_fc, conv,\n",
        "                      # low_latency_conv, low_latency_svdf, tiny_embedding_conv\n",
        "MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv,\n",
        "\n",
        "# Constants used during training only\n",
        "VERBOSITY = 'DEBUG'\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "EVAL_STEP_INTERVAL = '1000'\n",
        "SAVE_STEP_INTERVAL = '1000'\n",
        "# EVAL_STEP_INTERVAL = '100'\n",
        "# SAVE_STEP_INTERVAL = '100'\n",
        "\n",
        "# Constants for training directories and filepaths\n",
        "# DATASET_DIR =  'dataset/'\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
        "\n",
        "# Constants for inference directories and filepaths\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
        "\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rLYpvtg9P4o"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed_XpUrU5DvY"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Ty5mR58E4i"
      },
      "source": [
        "**DELETE** any old data from previous runs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APGx0fEh7hFF"
      },
      "source": [
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "# !rm -rf {DATASET_DIR} {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}\n",
        "!rm -rf {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfEUlfFBizio"
      },
      "source": [
        "Clone the TensorFlow Github Repository, which contains the relevant code required to run this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZArmzT85SLq"
      },
      "source": [
        "!git clone -q --depth 1 https://github.com/tensorflow/tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS9swHLSi7Bi"
      },
      "source": [
        "Load TensorBoard to visualize the accuracy and loss as training proceeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "q4qF1VxP3UE4",
        "outputId": "1f58aa19-c75a-41d3-c8b6-0c1fd7b1f3b1"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOGS_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1J96Ron-O4R"
      },
      "source": [
        "## Training\n",
        "\n",
        "The following script downloads the dataset and begin training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VJsEZx6lynbY"
      },
      "source": [
        "''' Modified by iwatake2222 ----------------------------------------------- '''\n",
        "# !python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "# --data_dir={DATASET_DIR} \\\n",
        "# --wanted_words={WANTED_WORDS} \\\n",
        "# --silence_percentage={SILENT_PERCENTAGE} \\\n",
        "# --unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "# --preprocess={PREPROCESS} \\\n",
        "# --window_stride={WINDOW_STRIDE} \\\n",
        "# --model_architecture={MODEL_ARCHITECTURE} \\\n",
        "# --how_many_training_steps={TRAINING_STEPS} \\\n",
        "# --learning_rate={LEARNING_RATE} \\\n",
        "# --train_dir={TRAIN_DIR} \\\n",
        "# --summaries_dir={LOGS_DIR} \\\n",
        "# --verbosity={VERBOSITY} \\\n",
        "# --eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "# --save_step_interval={SAVE_STEP_INTERVAL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "--data_url=\"\" \\\n",
        "--data_dir={DATASET_DIR} \\\n",
        "--wanted_words={WANTED_WORDS} \\\n",
        "--silence_percentage={SILENT_PERCENTAGE} \\\n",
        "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "--preprocess={PREPROCESS} \\\n",
        "--clip_duration_ms={CLIP_DURATION} \\\n",
        "--window_size_ms={WINDOW_SIZE} \\\n",
        "--window_stride={WINDOW_STRIDE} \\\n",
        "--feature_bin_count={FEATURE_BIN_COUNT} \\\n",
        "--model_architecture={MODEL_ARCHITECTURE} \\\n",
        "--how_many_training_steps={TRAINING_STEPS} \\\n",
        "--learning_rate={LEARNING_RATE} \\\n",
        "--train_dir={TRAIN_DIR} \\\n",
        "--summaries_dir={LOGS_DIR} \\\n",
        "--verbosity={VERBOSITY} \\\n",
        "--validation_percentage=10 \\\n",
        "--testing_percentage=0 \\\n",
        "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "--save_step_interval={SAVE_STEP_INTERVAL} \n",
        "# \\\n",
        "# --start_checkpoint=./train/conv.ckpt-3000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UczQKtqLi7OJ"
      },
      "source": [
        "# Skipping the training\n",
        "\n",
        "If you don't want to spend an hour or two training the model from scratch, you can download pretrained checkpoints by uncommenting the lines below (removing the '#'s at the start of each line) and running them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZw3VNlnla-J"
      },
      "source": [
        "#!curl -O \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_micro_train_2020_05_10.tgz\"\n",
        "#!tar xzf speech_micro_train_2020_05_10.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUJLrdS-ftl"
      },
      "source": [
        "## Generate a TensorFlow Model for Inference\n",
        "\n",
        "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyc3_eLh9sAg"
      },
      "source": [
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "# !rm -rf {SAVED_MODEL}\n",
        "# !python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "# --wanted_words=$WANTED_WORDS \\\n",
        "# --window_stride_ms=$WINDOW_STRIDE \\\n",
        "# --preprocess=$PREPROCESS \\\n",
        "# --model_architecture=$MODEL_ARCHITECTURE \\\n",
        "# --start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "# --save_format=saved_model \\\n",
        "# --output_file={SAVED_MODEL}\n",
        "!rm -rf {SAVED_MODEL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--clip_duration_ms=$CLIP_DURATION \\\n",
        "--clip_stride_ms=$WINDOW_SIZE \\\n",
        "--window_size_ms=$WINDOW_SIZE \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--feature_bin_count=$FEATURE_BIN_COUNT \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBGDxVI-nKG"
      },
      "source": [
        "## Generate a TensorFlow Lite Model\n",
        "\n",
        "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices.\n",
        "\n",
        "The following cell will also print the model size, which will be under 20 kilobytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIitkqvGWmre"
      },
      "source": [
        "import sys\n",
        "# We add this path so we can import the speech processing modules.\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzqECqMxgBh4"
      },
      "source": [
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "SAMPLE_RATE = 16000\n",
        "# CLIP_DURATION_MS = 1000\n",
        "# WINDOW_SIZE_MS = 30.0\n",
        "# FEATURE_BIN_COUNT = 40\n",
        "# BACKGROUND_FREQUENCY = 0.8\n",
        "BACKGROUND_FREQUENCY = 0.0\n",
        "BACKGROUND_VOLUME_RANGE = 0.1\n",
        "TIME_SHIFT_MS = 100.0\n",
        " \n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "# DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "DATA_URL = ''\n",
        "# VALIDATION_PERCENTAGE = 10\n",
        "# TESTING_PERCENTAGE = 10\n",
        "VALIDATION_PERCENTAGE = 0\n",
        "# use 100% causes error (AudioProcessor needs at least 1 trainign data)\n",
        "TESTING_PERCENTAGE = 99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQdAplJV1fz"
      },
      "source": [
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION, WINDOW_SIZE,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "# audio_processor = input_data.AudioProcessor(\n",
        "#     DATA_URL, DATASET_DIR,\n",
        "#     SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "#     WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "#     TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_TEST_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBj_AyCh1cC0"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        " \n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.inference_input_type = tf.lite.constants.INT8\n",
        "  converter.inference_output_type = tf.lite.constants.INT8\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(1000):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY, \n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      ''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "      # flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, int(40 * (CLIP_DURATION / 20 - 1)))\n",
        "      yield [flattened_data]\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLiDZTbLkzv"
      },
      "source": [
        "## Testing the TensorFlow Lite model's accuracy\n",
        "\n",
        "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQsEteKRLryJ"
      },
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
        "  # Load test data\n",
        "  np.random.seed(0) # set random seed for reproducible test results.\n",
        "  with tf.Session() as sess:\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        " \n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        " \n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        " \n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        " \n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "\n",
        "    ''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "    # To avoid \"interpreter.invoke() There is at least 1 reference to internal data\" error\n",
        "    try: \n",
        "        interpreter.invoke()\n",
        "    except:\n",
        "        interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "        interpreter.allocate_tensors()\n",
        "        interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        " \n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-pD52Na6jRa",
        "outputId": "f45c70cd-54a5-441a-eca5-22d4076d4077"
      },
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference(FLOAT_MODEL_TFLITE)\n",
        " \n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference(MODEL_TFLITE, model_type='Quantized')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Float model accuracy is 96.961726% (Number of test samples=7603)\n",
            "Quantized model accuracy is 96.988031% (Number of test samples=7603)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6Zqbxu-wIi"
      },
      "source": [
        "## Generate a TensorFlow Lite for MicroControllers Model\n",
        "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XohZOTjR8ZyE"
      },
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQnN0i_-0L2"
      },
      "source": [
        "## Deploy to a Microcontroller\n",
        "\n",
        "Follow the instructions in the [micro_speech](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) README.md for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview) to deploy this model on a specific microcontroller.\n",
        "\n",
        "**Reference Model:** If you have not modified this notebook, you can follow the instructions as is, to deploy the model. Refer to the [`micro_speech/train/models`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/models) directory to access the models generated in this notebook.\n",
        "\n",
        "**New Model:** If you have generated a new model to identify different words: (i) Update `kCategoryCount` and `kCategoryLabels` in [`micro_speech/micro_features/micro_model_settings.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h) and (ii) Update the values assigned to the variables defined in [`micro_speech/micro_features/model.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/micro_features/model.cc) with values displayed after running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYyh0VU8pca"
      },
      "source": [
        "# Print the C source file\n",
        "# !cat {MODEL_TFLITE_MICRO}\n",
        "\n",
        "''' Modified by iwatake2222 ---------------------------------------------- '''\n",
        "!tar czvf models.tgz models"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}